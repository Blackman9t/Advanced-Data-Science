{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_ml_pipelines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lawrence-Krukrubo/Advanced-Data-Science/blob/master/spark_ml_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY"
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "This installs Apache Spark 2.4.7, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhBhrGmyAvs"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q  http://apache.osuosl.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Kjvk_h1AHl"
      },
      "source": [
        "# Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnb_ePUyQIL"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P"
      },
      "source": [
        "# Start a SparkSession\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBUsOabyDqB7"
      },
      "source": [
        "First, let's initialise a spark context if none exists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgReRGl0y23D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b1a810-1906-402e-f4e5-fa2718dc99ec"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "try:\n",
        "    conf = SparkConf().setMaster(\"local\").setAppName(\"My_App\")\n",
        "    sc = SparkContext(conf = conf)\n",
        "    print('SparkContext Initialised Successfully!')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SparkContext Initialised Successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeS-wVHuDxx4"
      },
      "source": [
        "Then start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "U1cZv5vmQehd",
        "outputId": "0fcb31d1-a564-4055-cfdb-03e2d95bc602"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('MyApp').getOrCreate()\n",
        "spark"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6cb3c2e3b9dd:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.7</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>My_App</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f38c790c470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRe0be8UROrn"
      },
      "source": [
        "A Pipeline is a very convenient process of designing our data preprocessing in Machine Learning flow.<br>There are certain steps which we must do before the actual ML begins. These steps are called data-preprocessing and/or feature engineering.<br>The cool thing about pipelines is that we get some sort of a recipe or list of predefined steps already for us.<br> These steps could include:<br>1. Assigning categorical values e.g 0 or 1<br>2. Normalising the range of values per dimension<br>3. One-hot encoding and then the final<br>4. Modeling... where we train our ML algorithm.<br>\n",
        "So the idea is when using pipelines, we can maintain the same preprocessing and just switch out different modeling algorithnms or different parameter sets of the modeling algorithm without changing anything before. This is very very handy.<br>The overall idea of pipelines is that we can fuse our complete data processing flow into one single pipeline and that single pipeline we can further use downstream.<br>\n",
        "So the pipeline as a Machine Learning Algorithm has functions or methods which are called fit, evaluate and score. Fit basically starts the training, and score gives you back the predicted value.<br>\n",
        "One advantage is that we can cross-validate, that is you can try out many many parameters using that same very pipeline. And this really accelerates optimisation of the algorithm.<br>\n",
        "So in summary, pipelines are really facilitating our day to day work in machine learning as we can draw from pre-defined data processing steps, we make sure everything is aligned and we can switch and swap our algorithms as needed. We can create a pipeline and we can use this pipeline in downstream data processing in a process called hyperparameter-tuning for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nmSyfxdRkZn"
      },
      "source": [
        "Finally, remember that Dataframes in Apache Spark are always lazy in the sense that if you don't read the data nothing gets executed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H84J4IAeRxYv"
      },
      "source": [
        "<h3>1. Data Extraction</h3>\n",
        "\n",
        "* Note that the parquet file format uses compression and column store and actually maps data layout to the Apache Spark Tungsten memory layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZkw_gPEQvId"
      },
      "source": [
        "# This is the dataset that contains the different folders for reading the accelerometer data\n",
        "# We will clone this data set\n",
        "accelerometer_readings = 'https://github.com/wchill/HMP_Dataset.git'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFy0JyBoSHJM",
        "outputId": "5bdf8465-6dd2-4a0c-a31d-70bef12a16ad"
      },
      "source": [
        "!git clone https://github.com/wchill/HMP_Dataset.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'HMP_Dataset'...\n",
            "remote: Enumerating objects: 865, done.\u001b[K\n",
            "remote: Total 865 (delta 0), reused 0 (delta 0), pack-reused 865\u001b[K\n",
            "Receiving objects: 100% (865/865), 1010.96 KiB | 13.85 MiB/s, done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOBfaLk7SKnU",
        "outputId": "f49a781f-5f84-423b-b31e-3bb346cc1a33"
      },
      "source": [
        "# Let's list out the folders in the HMP_Dataset\n",
        "!ls HMP_Dataset"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Brush_teeth\tDrink_glass  Getup_bed\t  Pour_water\t Use_telephone\n",
            "Climb_stairs\tEat_meat     impdata.py   README.txt\t Walk\n",
            "Comb_hair\tEat_soup     Liedown_bed  Sitdown_chair\n",
            "Descend_stairs\tfinal.py     MANUAL.txt   Standup_chair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-O54id0Qj_I",
        "outputId": "1fe989ae-7d62-47bc-d5e9-9431b0c37710"
      },
      "source": [
        "# Let's have a look at one of the folders\n",
        "!ls HMP_Dataset/Brush_teeth"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt\n",
            "Accelerometer-2011-04-11-13-29-54-brush_teeth-f1.txt\n",
            "Accelerometer-2011-05-30-08-35-11-brush_teeth-f1.txt\n",
            "Accelerometer-2011-05-30-09-36-50-brush_teeth-f1.txt\n",
            "Accelerometer-2011-05-30-10-34-16-brush_teeth-m1.txt\n",
            "Accelerometer-2011-05-30-21-10-57-brush_teeth-f1.txt\n",
            "Accelerometer-2011-05-30-21-55-04-brush_teeth-m2.txt\n",
            "Accelerometer-2011-05-31-15-16-47-brush_teeth-f1.txt\n",
            "Accelerometer-2011-06-02-10-42-22-brush_teeth-f1.txt\n",
            "Accelerometer-2011-06-02-10-45-50-brush_teeth-f1.txt\n",
            "Accelerometer-2011-06-06-10-45-27-brush_teeth-f1.txt\n",
            "Accelerometer-2011-06-06-10-48-05-brush_teeth-f1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qASOgAD81Jjw"
      },
      "source": [
        "Let's see the content or structure of the data in these text files, looking at the Brush_teeth folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HCWM3tyQpvv",
        "outputId": "7a49665d-490f-4d36-b037-1a2618f92a61"
      },
      "source": [
        "!head HMP_Dataset/Brush_teeth/Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22 49 35\n",
            "22 49 35\n",
            "22 52 35\n",
            "22 52 35\n",
            "21 52 34\n",
            "22 51 34\n",
            "20 50 35\n",
            "22 52 34\n",
            "22 50 34\n",
            "22 51 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTTxDkxPQonW"
      },
      "source": [
        "let's recursively traverse through these folders in HMP_Dataset and create Apache spark DataFrame from the text files and then we just union all dataframes into one overall DataFrame containing all the data.\n",
        "\n",
        "Let's define the schema of the data frame below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpR0H-EDkpIl"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "\n",
        "schema = StructType([\n",
        "                     StructField('x',IntegerType(),True),\n",
        "                     StructField('y',IntegerType(),True),\n",
        "                     StructField('z',IntegerType(),True)\n",
        "])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh9jbL5Un8Rc"
      },
      "source": [
        "Now let's traverse through the data using the OS library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2-a91fBoELc",
        "outputId": "10a89c25-e37d-4d6f-977b-76b3682b1e9d"
      },
      "source": [
        "file_list = os.listdir('HMP_Dataset')\n",
        "file_list"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Drink_glass',\n",
              " 'final.py',\n",
              " 'Getup_bed',\n",
              " 'Descend_stairs',\n",
              " 'Comb_hair',\n",
              " 'Sitdown_chair',\n",
              " 'Liedown_bed',\n",
              " '.idea',\n",
              " 'README.txt',\n",
              " 'MANUAL.txt',\n",
              " 'Walk',\n",
              " 'Eat_soup',\n",
              " 'Pour_water',\n",
              " '.git',\n",
              " 'Standup_chair',\n",
              " 'Eat_meat',\n",
              " 'Climb_stairs',\n",
              " 'Brush_teeth',\n",
              " 'impdata.py',\n",
              " 'Use_telephone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCVhMibspSbv"
      },
      "source": [
        "Now let's get rid of the folders that do not contain underscores as we don't need those"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRgzGpyppZwi",
        "outputId": "f84797b8-a396-4183-dba5-c6ec6d0f613a"
      },
      "source": [
        "file_list_filtered = [i for i in file_list if '_' in i or i == 'Walk']\n",
        "file_list_filtered"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Drink_glass',\n",
              " 'Getup_bed',\n",
              " 'Descend_stairs',\n",
              " 'Comb_hair',\n",
              " 'Sitdown_chair',\n",
              " 'Liedown_bed',\n",
              " 'Walk',\n",
              " 'Eat_soup',\n",
              " 'Pour_water',\n",
              " 'Standup_chair',\n",
              " 'Eat_meat',\n",
              " 'Climb_stairs',\n",
              " 'Brush_teeth',\n",
              " 'Use_telephone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbS0hTVsrNmy"
      },
      "source": [
        "Okay so we have all the folders containing data in one array. Now we can iterate over this array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i6pYp4rrOfF",
        "outputId": "2c5ea374-13cc-4231-a7ea-123d25bf8b0e"
      },
      "source": [
        "# First we define an empty data frame that we'd append data to\n",
        "df = None\n",
        "# next we import tqdm progress bars to see how our code runs \n",
        "from tqdm import tqdm\n",
        "\n",
        "from pyspark.sql.functions import lit\n",
        "# The lit library helps us write string literals column to an apache dataframe.\n",
        "\n",
        "# Now let's iterate through the folders\n",
        "for category in tqdm(file_list_filtered):\n",
        "    # Now we traverse all through the files in each folder\n",
        "    data_files = os.listdir('HMP_Dataset/' + category)\n",
        "    for data_file in data_files:\n",
        "        # Now we create a temporary dataframe\n",
        "        # we use our defined schema above\n",
        "        temp_df = spark.read.option('header','false').option('delimiter',' ').csv('HMP_Dataset/'+ category + '/' + data_file, schema=schema)  \n",
        "        temp_df = temp_df.withColumn('class',lit(category))  # Adding a class column to the dataframe\n",
        "        temp_df = temp_df.withColumn('source',lit(data_file))  # Adding a source column to the dataframe\n",
        "        # now we put a condition if df is empty\n",
        "        if df is None:\n",
        "            df = temp_df\n",
        "        else:\n",
        "            df = df.union(temp_df)  # else union appends the data frames vertically"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:42<00:00,  3.04s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHH4EnaYt2Nz"
      },
      "source": [
        "Let's see the schema of our DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl-HlVCwt5ob",
        "outputId": "781159d6-b006-4ab8-d5f0-369d1eaa547e"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- x: integer (nullable = true)\n",
            " |-- y: integer (nullable = true)\n",
            " |-- z: integer (nullable = true)\n",
            " |-- class: string (nullable = false)\n",
            " |-- source: string (nullable = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXbSCjMkuegQ"
      },
      "source": [
        "Let's see the first 10 rows of the DataFrame..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnNmMhg6uie2",
        "outputId": "c7ab0d4d-3d7c-44e6-d4e5-a5ee0bd9b338"
      },
      "source": [
        "df.show(10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-----------+--------------------+\n",
            "|  x|  y|  z|      class|              source|\n",
            "+---+---+---+-----------+--------------------+\n",
            "| 29| 39| 51|Drink_glass|Accelerometer-201...|\n",
            "| 29| 39| 51|Drink_glass|Accelerometer-201...|\n",
            "| 28| 38| 52|Drink_glass|Accelerometer-201...|\n",
            "| 29| 37| 51|Drink_glass|Accelerometer-201...|\n",
            "| 30| 38| 52|Drink_glass|Accelerometer-201...|\n",
            "| 29| 39| 52|Drink_glass|Accelerometer-201...|\n",
            "| 30| 39| 51|Drink_glass|Accelerometer-201...|\n",
            "| 29| 39| 52|Drink_glass|Accelerometer-201...|\n",
            "| 29| 38| 51|Drink_glass|Accelerometer-201...|\n",
            "| 30| 39| 52|Drink_glass|Accelerometer-201...|\n",
            "+---+---+---+-----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NOPo1Q6x4be"
      },
      "source": [
        "<h3>2. Data Transformation</h3>\n",
        "\n",
        "Now we need to transform the data and create an integer representation of the class column as ML algorithms cannot cope with a String. So we will transform the class into a number of integers. using the StringIndexer module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG7pRAhqyBzZ",
        "outputId": "446131c8-6e8b-4a4d-da4b-56f2f11e9a5c"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol = 'class', outputCol = 'classIndex')\n",
        "indexed = indexer.fit(df).transform(df)  # This is a new data frame\n",
        "\n",
        "# Let's see it\n",
        "indexed.show(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-----------+--------------------+----------+\n",
            "|  x|  y|  z|      class|              source|classIndex|\n",
            "+---+---+---+-----------+--------------------+----------+\n",
            "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|\n",
            "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|\n",
            "| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|\n",
            "| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|\n",
            "| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|\n",
            "| 29| 39| 52|Drink_glass|Accelerometer-201...|       2.0|\n",
            "| 30| 39| 51|Drink_glass|Accelerometer-201...|       2.0|\n",
            "| 29| 39| 52|Drink_glass|Accelerometer-201...|       2.0|\n",
            "| 29| 38| 51|Drink_glass|Accelerometer-201...|       2.0|\n",
            "| 30| 39| 52|Drink_glass|Accelerometer-201...|       2.0|\n",
            "+---+---+---+-----------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZnrJfir08Ar"
      },
      "source": [
        "<h3>3. One-Hot Encoding:</h3>\n",
        "\n",
        "We can see the class index for each class. Good.\n",
        "So now we do one-hot-encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH3NZz-W1C6z",
        "outputId": "846800a3-9efd-4948-c61f-1fc20959bd2b"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "# The OneHotEncoder is a pure transformer object. it does not use the fit()\n",
        "encoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'categoryVec')\n",
        "encoded = encoder.transform(indexed)  # This is a new data frame\n",
        "encoded.show(10, False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n",
            "|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |\n",
            "+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n",
            "|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "|29 |39 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "|30 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "|29 |39 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "|29 |38 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "|30 |39 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|\n",
            "+---+---+---+-----------+----------------------------------------------------+----------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M1ME9Y8AUyQ"
      },
      "source": [
        "<h3>4. VectorAssembler</h3>\n",
        "\n",
        "Next thing we need to do is to transform our values X, Y, Z into vectors because sparkML only can work on vector objects.\n",
        "So let's import vectors and vectorAssembler libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4zLc7qSAKDe",
        "outputId": "cad0fc3e-604f-46c2-910c-6278531ed9dc"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "# VectorAssembler creates vectors from ordinary data types for us\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols = ['x','y','z'], outputCol = 'features')\n",
        "# Now we use the vectorAssembler object to transform our last updated dataframe\n",
        "\n",
        "features_vectorized = vectorAssembler.transform(encoded)  # note this is a new df\n",
        "\n",
        "# Let's see the first 10 rows\n",
        "features_vectorized.show(10, False)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n",
            "|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |features        |\n",
            "+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n",
            "|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|\n",
            "|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|\n",
            "|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[28.0,38.0,52.0]|\n",
            "|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,37.0,51.0]|\n",
            "|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[30.0,38.0,52.0]|\n",
            "|29 |39 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,52.0]|\n",
            "|30 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[30.0,39.0,51.0]|\n",
            "|29 |39 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,52.0]|\n",
            "|29 |38 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,38.0,51.0]|\n",
            "|30 |39 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[30.0,39.0,52.0]|\n",
            "+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOpmQsOME6oN"
      },
      "source": [
        "<h3>5. Normalizing The Dataset</h3>\n",
        "\n",
        "So the next thing we do now is Normalising the data set.\n",
        "This makes the range of values in the data set to be between 0 and 1 or -1 and 1 sometimes. The idea is to have all features data within the same range so no one over shadows the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91OHD--4MuYX"
      },
      "source": [
        "from pyspark.ml.feature import Normalizer\n",
        "normalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm', p=1.0)  # Manhattan Distance\n",
        "normalized_data = normalizer.transform(features_vectorized) # New data frame too."
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83P3T24VFB1-",
        "outputId": "8f2f408e-3d6f-46a1-e64e-358ac107842b"
      },
      "source": [
        "normalized_data.show(10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
            "|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n",
            "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
            "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n",
            "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n",
            "| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|\n",
            "| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|\n",
            "| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|\n",
            "| 29| 39| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,52.0]|[0.24166666666666...|\n",
            "| 30| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,39.0,51.0]|  [0.25,0.325,0.425]|\n",
            "| 29| 39| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,52.0]|[0.24166666666666...|\n",
            "| 29| 38| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,38.0,51.0]|[0.24576271186440...|\n",
            "| 30| 39| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,39.0,52.0]|[0.24793388429752...|\n",
            "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCvEjBtbYwmy"
      },
      "source": [
        "As seen in the features_norm column, all values have been squashed between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmzzW2n6xqAA"
      },
      "source": [
        "<h3>6. Creating The Pipeline:</h3>\n",
        "\n",
        "The Pipeline constructor below takes an array of Pipeline stages we pass to it.<br>\n",
        "Here we pass the 4 stages above in the right sequence one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVv7Kpz0Yxqz"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages = [indexer,encoder,vectorAssembler,normalizer])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBbnIr0N9oVO"
      },
      "source": [
        "Now let's fit the Pipeline object to our original data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhzG4qQk9o2c"
      },
      "source": [
        "data_model = pipeline.fit(df)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HCTPAmG-JUE"
      },
      "source": [
        "Finally let's transform our data frame using the Pipeline Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0CMdd5U-MIv"
      },
      "source": [
        "pipelined_data = data_model.transform(df)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lp1LTh9_D6G"
      },
      "source": [
        "Let's see the first 10 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqWzE9zb_IHb",
        "outputId": "87ed968c-d536-48a1-9a5b-64403f95b43d"
      },
      "source": [
        "pipelined_data.show(10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
            "|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n",
            "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
            "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n",
            "| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|\n",
            "| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|\n",
            "| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|\n",
            "| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|\n",
            "| 29| 39| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,52.0]|[0.24166666666666...|\n",
            "| 30| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,39.0,51.0]|  [0.25,0.325,0.425]|\n",
            "| 29| 39| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,52.0]|[0.24166666666666...|\n",
            "| 29| 38| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,38.0,51.0]|[0.24576271186440...|\n",
            "| 30| 39| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,39.0,52.0]|[0.24793388429752...|\n",
            "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxIl6CDmAPtI",
        "outputId": "500115fe-4431-4375-e408-76d7a6aec5eb"
      },
      "source": [
        "# first let's list out the columns we want to drop\n",
        "cols_to_drop = ['x','y','z','class','source','classIndex','features']\n",
        "\n",
        "# Next let's use a list comprehension with conditionals to select cols we need\n",
        "selected_cols = [col for col in pipelined_data.columns if col not in cols_to_drop]\n",
        "\n",
        "# Let's define a new train_df with only the categoryVec and features_norm cols\n",
        "df_train = pipelined_data.select(selected_cols)\n",
        "\n",
        "# Let's see our training dataframe.\n",
        "df_train.show(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+\n",
            "|   categoryVec|       features_norm|\n",
            "+--------------+--------------------+\n",
            "|(13,[2],[1.0])|[0.24369747899159...|\n",
            "|(13,[2],[1.0])|[0.24369747899159...|\n",
            "|(13,[2],[1.0])|[0.23728813559322...|\n",
            "|(13,[2],[1.0])|[0.24786324786324...|\n",
            "|(13,[2],[1.0])|[0.25,0.316666666...|\n",
            "|(13,[2],[1.0])|[0.24166666666666...|\n",
            "|(13,[2],[1.0])|  [0.25,0.325,0.425]|\n",
            "|(13,[2],[1.0])|[0.24166666666666...|\n",
            "|(13,[2],[1.0])|[0.24576271186440...|\n",
            "|(13,[2],[1.0])|[0.24793388429752...|\n",
            "+--------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}