{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkML_pipelines.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP/inhfSanPF+jXkx8kb6VB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blackman9t/Advanced-Data-Science/blob/master/SparkML_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEiFbU0QoXyc",
        "colab_type": "text"
      },
      "source": [
        "A Pipeline is a very convenient process of designing our data preprocessing in Machine Learning flow.<br>There are certain steps which we must do before the actual ML begins. These steps are called data-preprocessing and/or feature engineering.<br>The cool thing about pipelines is that we get some sort of a recipe or list of predefined steps already for us.<br> These steps could include:<br>1. Assigning categorical values e.g 0 or 1<br>2. Normalising the range of values per dimension<br>3. One-hot encoding and then the final<br>4. Modeling... where we train our ML algorithm.<br>\n",
        "So the idea is when using pipelines, we can maintain the same preprocessing and just switch out different modeling algorithnms or different parameter sets of the modeling algorithm without changing anything before. This is very very handy.<br>The overall idea of pipelines is that we can fuse our complete data processing flow into one single pipeline and that single pipeline we can further use downstream.<br>\n",
        "So the pipeline as a Machine Learning Algorithm has functions or methods which are called fit, evaluate and score. Fit basically starts the training, and score gives you back the predicted value.<br>\n",
        "One advantage is that we can cross-validate, that is you can try out many many parameters using that same very pipeline. And this really accelerates optimisation of the algorithm.<br>\n",
        "So in summary, pipelines are really facilitating our day to day work in machine learning as we can draw from pre-defined data processing steps, we make sure everything is aligned and we can switch and swap our algorithms as needed. We can create a pipeline and we can use this pipeline in downstream data processing in a process called hyperparameter-tuning for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26XfIBxA8rrV",
        "colab_type": "text"
      },
      "source": [
        "Finally, remember that Dataframes in Apache Spark are always lazy in the sense that if you don't read the data nothing gets executed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYL84ThsDqVc",
        "colab_type": "text"
      },
      "source": [
        "First let's load our spark dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhldBDX0nsei",
        "colab_type": "code",
        "outputId": "d228a75b-c972-4076-a1aa-328973072fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.4)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6NpGI1uEArX",
        "colab_type": "text"
      },
      "source": [
        "Now let's initialise a spark context if none exists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOLeLa6NEKFl",
        "colab_type": "code",
        "outputId": "3c1bfb86-2cc6-432e-df90-eae14d786051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "try:\n",
        "    conf = SparkConf().setMaster(\"local\").setAppName(\"My_App\")\n",
        "    sc = SparkContext(conf = conf)\n",
        "    print('SparkContext Initialised Successfully!')\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cannot run multiple SparkContexts at once; existing SparkContext(app=My_App, master=local) created by __init__ at <ipython-input-3-e82698b8cac6>:4 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv0E_OIQItTP",
        "colab_type": "code",
        "outputId": "2b398e69-7035-403e-fa02-bb800e15bdef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "sc"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://80f2b1314d34:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>My_App</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local appName=My_App>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDwtfcGsIwQM",
        "colab_type": "text"
      },
      "source": [
        "Next let's initialise a spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FouewldjJAVO",
        "colab_type": "code",
        "outputId": "eced19a9-9c76-40c5-b0cc-20f2f22541a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('My_App').getOrCreate()\n",
        "spark"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://80f2b1314d34:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>My_App</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f745728e5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJSY263TJjJB",
        "colab_type": "text"
      },
      "source": [
        "## Intro to SparkML\n",
        "\n",
        "Note that the parquet file format uses compression and column store and actually maps data layout to the Apache Spark Tungsten memory layout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oid3f6pb4Pnm",
        "colab_type": "text"
      },
      "source": [
        "### 1. Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGJ08QJCJptw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the dataset that contains the different folders for reading the accelerometer data\n",
        "# We will clone this data set\n",
        "accelerometer_readings = 'https://github.com/wchill/HMP_Dataset.git'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDXA9ucPirp7",
        "colab_type": "code",
        "outputId": "b2ecf731-265a-41a2-d2c6-92fdeaae7bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/wchill/HMP_Dataset.git"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'HMP_Dataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhtkrPG8jX0x",
        "colab_type": "code",
        "outputId": "c4750a7f-1d91-4110-aeb8-b87442b2832d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Let's list out the folders in the HMP_Dataset\n",
        "!ls HMP_Dataset"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Brush_teeth\tDrink_glass  Getup_bed\t  Pour_water\t Use_telephone\n",
            "Climb_stairs\tEat_meat     impdata.py   README.txt\t Walk\n",
            "Comb_hair\tEat_soup     Liedown_bed  Sitdown_chair\n",
            "Descend_stairs\tfinal.py     MANUAL.txt   Standup_chair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Voi65yj1qm",
        "colab_type": "code",
        "outputId": "bd6d5ffe-3425-43c3-bd50-7613c11d9f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "# Let's have a look at one of the folders\n",
        "!ls HMP_Dataset/Brush_teeth"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt\n",
            "Accelerometer-2011-04-11-13-29-54-brush_teeth-f1.txt\n",
            "Accelerometer-2011-05-30-08-35-11-brush_teeth-f1.txt\n",
            "Accelerometer-2011-05-30-09-36-50-brush_teeth-f1.txt\n",
            "Accelerometer-2011-05-30-10-34-16-brush_teeth-m1.txt\n",
            "Accelerometer-2011-05-30-21-10-57-brush_teeth-f1.txt\n",
            "Accelerometer-2011-05-30-21-55-04-brush_teeth-m2.txt\n",
            "Accelerometer-2011-05-31-15-16-47-brush_teeth-f1.txt\n",
            "Accelerometer-2011-06-02-10-42-22-brush_teeth-f1.txt\n",
            "Accelerometer-2011-06-02-10-45-50-brush_teeth-f1.txt\n",
            "Accelerometer-2011-06-06-10-45-27-brush_teeth-f1.txt\n",
            "Accelerometer-2011-06-06-10-48-05-brush_teeth-f1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlsacUfQkUn2",
        "colab_type": "text"
      },
      "source": [
        "let's recursively traverse through those folders in HMP_Dataset and create Apache spark DataFrame from those files and then we just union all dataframes into one overall DataFrame containing all the data.<br>\n",
        "Let's define the schema of the data frame below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8OXrpoZlX3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "\n",
        "schema = StructType([\n",
        "                     StructField('x',IntegerType(),True),\n",
        "                     StructField('y',IntegerType(),True),\n",
        "                     StructField('z',IntegerType(),True)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NiAvBkbmUT9",
        "colab_type": "text"
      },
      "source": [
        "Now let's import OS for traversing through the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_WxO-T3mgDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toBPCdtQmoTq",
        "colab_type": "code",
        "outputId": "492534d6-dfde-4eb4-a19a-cce23d418ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "file_list = os.listdir('HMP_Dataset')\n",
        "file_list"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sitdown_chair',\n",
              " '.idea',\n",
              " 'Liedown_bed',\n",
              " 'impdata.py',\n",
              " 'Climb_stairs',\n",
              " 'Use_telephone',\n",
              " 'Comb_hair',\n",
              " 'Standup_chair',\n",
              " 'Eat_soup',\n",
              " 'README.txt',\n",
              " 'Pour_water',\n",
              " 'Eat_meat',\n",
              " 'Descend_stairs',\n",
              " 'Drink_glass',\n",
              " 'MANUAL.txt',\n",
              " '.git',\n",
              " 'final.py',\n",
              " 'Walk',\n",
              " 'Getup_bed',\n",
              " 'Brush_teeth']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIXJmgkzm3F2",
        "colab_type": "text"
      },
      "source": [
        "Now let's get rid of the folders that do not contain underscores as we don't need those"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7dW4rxQnAPd",
        "colab_type": "code",
        "outputId": "bdc639e2-3ab7-44f0-c123-b45cdd88b8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "file_list_filtered = [x for x in file_list if '_' in x]\n",
        "file_list_filtered"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sitdown_chair',\n",
              " 'Liedown_bed',\n",
              " 'Climb_stairs',\n",
              " 'Use_telephone',\n",
              " 'Comb_hair',\n",
              " 'Standup_chair',\n",
              " 'Eat_soup',\n",
              " 'Pour_water',\n",
              " 'Eat_meat',\n",
              " 'Descend_stairs',\n",
              " 'Drink_glass',\n",
              " 'Getup_bed',\n",
              " 'Brush_teeth']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twQXDHgpqmqn",
        "colab_type": "text"
      },
      "source": [
        "Okay so we have all the folders containing data in one array. Now we can iterate over this array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFLN3l0Bqvel",
        "colab_type": "code",
        "outputId": "0cc0813c-83b5-46d1-840d-16e8d18acac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "# First we define an empty data frame that we'd append data to\n",
        "df = None\n",
        "# next we import tqdm progress bars to see how our code runs \n",
        "from tqdm import tqdm\n",
        "\n",
        "from pyspark.sql.functions import lit\n",
        "# The lit library helps us write string literals column to an apache dataframe.\n",
        "\n",
        "# Now let's iterate through the folders\n",
        "for category in tqdm(file_list_filtered):\n",
        "    # Now we traverse all through the files in each folder\n",
        "    data_files = os.listdir('HMP_Dataset/' + category)\n",
        "    for data_file in data_files:\n",
        "        # first let's print it to be sure where we are\n",
        "        #print(data_file)\n",
        "        # Now we create a temporary dataframe\n",
        "        temp_df = spark.read.option('header','false').option('delimiter',' ').csv('HMP_Dataset/'+ category + '/' + data_file, schema=schema)  # we use our defined schema above\n",
        "        temp_df = temp_df.withColumn('class',lit(category))  # Adding a class column to the dataframe\n",
        "        temp_df = temp_df.withColumn('source',lit(data_file))  # Adding a source column to the dataframe\n",
        "        # now we put a condition if df is empty\n",
        "        if df is None:\n",
        "            df = temp_df\n",
        "        else:\n",
        "            df = df.union(temp_df)  # else union appends the data frames vertically\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8%|▊         | 1/13 [00:04<00:56,  4.73s/it]\u001b[A\u001b[A\n",
            "\n",
            " 15%|█▌        | 2/13 [00:06<00:41,  3.78s/it]\u001b[A\u001b[A\n",
            "\n",
            " 23%|██▎       | 3/13 [00:12<00:44,  4.48s/it]\u001b[A\u001b[A\n",
            "\n",
            " 31%|███       | 4/13 [00:13<00:30,  3.37s/it]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 5/13 [00:15<00:23,  2.95s/it]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▌     | 6/13 [00:22<00:29,  4.17s/it]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▍    | 7/13 [00:22<00:17,  2.98s/it]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 8/13 [00:29<00:21,  4.35s/it]\u001b[A\u001b[A\n",
            "\n",
            " 69%|██████▉   | 9/13 [00:30<00:12,  3.16s/it]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 10/13 [00:33<00:09,  3.27s/it]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▍ | 11/13 [00:42<00:09,  4.90s/it]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 12/13 [00:52<00:06,  6.33s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 13/13 [00:53<00:00,  4.81s/it]\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U38GlNb2wLZ",
        "colab_type": "text"
      },
      "source": [
        "Let's see the dataframe created from all the files in those folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrbSiqdP216A",
        "colab_type": "code",
        "outputId": "2a21cdcb-62d5-41bd-d5c1-b35d484004cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------------+--------------------+\n",
            "|  x|  y|  z|        class|              source|\n",
            "+---+---+---+-------------+--------------------+\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|\n",
            "| 12| 36| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 12| 36| 36|Sitdown_chair|Accelerometer-201...|\n",
            "| 11| 38| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 13| 38| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 13| 37| 34|Sitdown_chair|Accelerometer-201...|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 13| 38| 34|Sitdown_chair|Accelerometer-201...|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|\n",
            "| 12| 38| 34|Sitdown_chair|Accelerometer-201...|\n",
            "| 14| 36| 36|Sitdown_chair|Accelerometer-201...|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|\n",
            "| 12| 38| 36|Sitdown_chair|Accelerometer-201...|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|\n",
            "+---+---+---+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpfAvc2y3TVE",
        "colab_type": "text"
      },
      "source": [
        "Romeo Keinzler usually creates a notebook that does this exercise and he calls it ETL<br>\n",
        "It means Extract, Transform and Load data to a spark dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5eyc7ls4XLr",
        "colab_type": "text"
      },
      "source": [
        "### 2. Data Transformation\n",
        "\n",
        "Now we need to transform the data and create an integer representation of the class column as ML algorithms cannot cope with a string. So we will transform the class to a number of integers. using the StringIndexer module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BntmfVaZ4cDX",
        "colab_type": "code",
        "outputId": "8967a99d-dabd-437a-a646-c08979a440ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol = 'class', outputCol = 'classIndex')\n",
        "indexed = indexer.fit(df).transform(df)  # This is a new data frame\n",
        "\n",
        "# Let's see it\n",
        "indexed.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------------+--------------------+----------+\n",
            "|  x|  y|  z|        class|              source|classIndex|\n",
            "+---+---+---+-------------+--------------------+----------+\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 12| 36| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 12| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 11| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 13| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 13| 37| 34|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 13| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 12| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 14| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 12| 38| 36|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|\n",
            "+---+---+---+-------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ6SdU8s7VZ5",
        "colab_type": "text"
      },
      "source": [
        "We can see the class index for each class. Good.<br>\n",
        "So now we do one-hot-encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irNqgIg17evD",
        "colab_type": "code",
        "outputId": "65b812c0-d16f-437d-b1a4-265c0d7dcc2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "# The OneHotEncoder is a pure transformer object. it does not use the fit()\n",
        "encoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'categoryVec')\n",
        "encoded = encoder.transform(indexed)  # This is a new data frame\n",
        "encoded.show()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------------+--------------------+----------+--------------+\n",
            "|  x|  y|  z|        class|              source|classIndex|   categoryVec|\n",
            "+---+---+---+-------------+--------------------+----------+--------------+\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 12| 36| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 12| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 11| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 13| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 13| 37| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 13| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 12| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 14| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 12| 38| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|\n",
            "+---+---+---+-------------+--------------------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--goxL4d9TcD",
        "colab_type": "text"
      },
      "source": [
        "next thing we need to do is to transform our values X, Y, Z into vectors because sparkML only can work on vector objects.<br>\n",
        "So let's import vectors and vectorAssembler libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBCmcIdk9o-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "# VectorAssembler creates vectors from ordinary data types for us\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols = ['x','y','z'], outputCol = 'features')\n",
        "# Now we use the vectorAssembler object to transform our last updated dataframe\n",
        "features_vectorized = vectorAssembler.transform(encoded)  # note this is a new df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6R2s_f-_t57",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "35a0cf03-3569-44dc-cc65-098fcaefb1d7"
      },
      "source": [
        "# Let's see the data\n",
        "features_vectorized.show()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------------+--------------------+----------+--------------+----------------+\n",
            "|  x|  y|  z|        class|              source|classIndex|   categoryVec|        features|\n",
            "+---+---+---+-------------+--------------------+----------+--------------+----------------+\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,36.0]|\n",
            "| 12| 36| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,36.0,35.0]|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,35.0]|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,35.0]|\n",
            "| 12| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,36.0,36.0]|\n",
            "| 11| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[11.0,38.0,35.0]|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,35.0]|\n",
            "| 13| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,38.0,35.0]|\n",
            "| 13| 37| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,34.0]|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,35.0]|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,35.0]|\n",
            "| 13| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,38.0,34.0]|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,35.0]|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,35.0]|\n",
            "| 12| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,34.0]|\n",
            "| 14| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[14.0,36.0,36.0]|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[11.0,37.0,36.0]|\n",
            "| 12| 38| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,36.0]|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[11.0,37.0,36.0]|\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,36.0]|\n",
            "+---+---+---+-------------+--------------------+----------+--------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inNTP9OTAEBD",
        "colab_type": "text"
      },
      "source": [
        "So we now have the features corresponding to columns x, y, z, but these now are an Apache spark vector object. Which is the correct object for ML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO9J5hlrAy8M",
        "colab_type": "text"
      },
      "source": [
        "So the next thing we do now is Normalising the data set.<br>\n",
        "This makes the range of values in the data set to be between 0 and 1 or -1 and 1 sometimes. The idea is to have all features data within the same range so no one over shadows the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSGrFzSWBOVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import Normalizer\n",
        "normalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm',p=1.0)\n",
        "normalized_data = normalizer.transform(features_vectorized) # New data frame too."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Oej_XG1GZmU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "2b8c6055-1a60-4fb4-d173-f9d7cd17dbcc"
      },
      "source": [
        "# Let's see the normalized data\n",
        "normalized_data.show()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------------+--------------------+----------+--------------+----------------+--------------------+\n",
            "|  x|  y|  z|        class|              source|classIndex|   categoryVec|        features|       features_norm|\n",
            "+---+---+---+-------------+--------------------+----------+--------------+----------------+--------------------+\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,36.0]|[0.14117647058823...|\n",
            "| 12| 36| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,36.0,35.0]|[0.14457831325301...|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,35.0]|[0.14117647058823...|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,35.0]|[0.15294117647058...|\n",
            "| 12| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,36.0,36.0]|[0.14285714285714...|\n",
            "| 11| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[11.0,38.0,35.0]|[0.13095238095238...|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,35.0]|[0.14117647058823...|\n",
            "| 13| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,38.0,35.0]|[0.15116279069767...|\n",
            "| 13| 37| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,34.0]|[0.15476190476190...|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,35.0]|[0.14285714285714...|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,35.0]|[0.15294117647058...|\n",
            "| 13| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,38.0,34.0]|[0.15294117647058...|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,35.0]|[0.15294117647058...|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,35.0]|[0.14285714285714...|\n",
            "| 12| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,34.0]|[0.14285714285714...|\n",
            "| 14| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[14.0,36.0,36.0]|[0.16279069767441...|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[11.0,37.0,36.0]|[0.13095238095238...|\n",
            "| 12| 38| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,36.0]|[0.13953488372093...|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[11.0,37.0,36.0]|[0.13095238095238...|\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,36.0]|[0.14117647058823...|\n",
            "+---+---+---+-------------+--------------------+----------+--------------+----------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Goal2EKnRa",
        "colab_type": "text"
      },
      "source": [
        "As seen in the features_norm column, all values have been squashed between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oV5yafOKSM2",
        "colab_type": "text"
      },
      "source": [
        "### Creating The Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oetoru2WKdnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "# The Pipeline constructor below takes an array of Pipeline stages we pass to it.\n",
        "# here we pass the 4 stages above in the right sequence one after another.\n",
        "pipeline = Pipeline(stages = [indexer,encoder,vectorAssembler,normalizer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejb_hwq3MNjd",
        "colab_type": "text"
      },
      "source": [
        "Now let's fit the Pipeline object to our original data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnmte1GmMVeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = pipeline.fit(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qopwFp7_MkfM",
        "colab_type": "text"
      },
      "source": [
        "Finally let's transform our data frame using the Pipeline Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8jnQxNvMq0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIUj-UwEMzTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "8009cc1f-0036-41ba-bb4b-b33d733285a4"
      },
      "source": [
        "# Let's see the first 20 rows\n",
        "prediction.show()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------------+--------------------+----------+--------------+----------------+--------------------+\n",
            "|  x|  y|  z|        class|              source|classIndex|   categoryVec|        features|       features_norm|\n",
            "+---+---+---+-------------+--------------------+----------+--------------+----------------+--------------------+\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,36.0]|[0.14117647058823...|\n",
            "| 12| 36| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,36.0,35.0]|[0.14457831325301...|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,35.0]|[0.14117647058823...|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,35.0]|[0.15294117647058...|\n",
            "| 12| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,36.0,36.0]|[0.14285714285714...|\n",
            "| 11| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[11.0,38.0,35.0]|[0.13095238095238...|\n",
            "| 12| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,35.0]|[0.14117647058823...|\n",
            "| 13| 38| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,38.0,35.0]|[0.15116279069767...|\n",
            "| 13| 37| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,34.0]|[0.15476190476190...|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,35.0]|[0.14285714285714...|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,35.0]|[0.15294117647058...|\n",
            "| 13| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,38.0,34.0]|[0.15294117647058...|\n",
            "| 13| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[13.0,37.0,35.0]|[0.15294117647058...|\n",
            "| 12| 37| 35|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,35.0]|[0.14285714285714...|\n",
            "| 12| 38| 34|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,34.0]|[0.14285714285714...|\n",
            "| 14| 36| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[14.0,36.0,36.0]|[0.16279069767441...|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[11.0,37.0,36.0]|[0.13095238095238...|\n",
            "| 12| 38| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,38.0,36.0]|[0.13953488372093...|\n",
            "| 11| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[11.0,37.0,36.0]|[0.13095238095238...|\n",
            "| 12| 37| 36|Sitdown_chair|Accelerometer-201...|       7.0|(12,[7],[1.0])|[12.0,37.0,36.0]|[0.14117647058823...|\n",
            "+---+---+---+-------------+--------------------+----------+--------------+----------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AanMPncNSGX",
        "colab_type": "text"
      },
      "source": [
        "So we see exactly the same data frame as created before from the individual stages have been created using the Pipeline function. <br>Now we can fit and transform our data in one go. This is a really handy function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tmTNQ63N5Gs",
        "colab_type": "text"
      },
      "source": [
        "Let's get rid of all the columns we don't need "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHZP0i-sNyqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "d1caa0d3-2c92-40a1-87de-03486f33abe0"
      },
      "source": [
        "# first let's list out the columns we want to drop\n",
        "cols_to_drop = ['x','y','z','class','source','classIndex','features']\n",
        "\n",
        "# Next let's use a list comprehension with conditionals to select cols we need\n",
        "selected_cols = [col for col in prediction.columns if col not in cols_to_drop]\n",
        "\n",
        "# Let's define a new train_df with only the categoryVec and features_norm cols\n",
        "df_train = prediction.select(selected_cols)\n",
        "\n",
        "# Let's see our training dataframe.\n",
        "df_train.show()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+\n",
            "|   categoryVec|       features_norm|\n",
            "+--------------+--------------------+\n",
            "|(12,[7],[1.0])|[0.14117647058823...|\n",
            "|(12,[7],[1.0])|[0.14457831325301...|\n",
            "|(12,[7],[1.0])|[0.14117647058823...|\n",
            "|(12,[7],[1.0])|[0.15294117647058...|\n",
            "|(12,[7],[1.0])|[0.14285714285714...|\n",
            "|(12,[7],[1.0])|[0.13095238095238...|\n",
            "|(12,[7],[1.0])|[0.14117647058823...|\n",
            "|(12,[7],[1.0])|[0.15116279069767...|\n",
            "|(12,[7],[1.0])|[0.15476190476190...|\n",
            "|(12,[7],[1.0])|[0.14285714285714...|\n",
            "|(12,[7],[1.0])|[0.15294117647058...|\n",
            "|(12,[7],[1.0])|[0.15294117647058...|\n",
            "|(12,[7],[1.0])|[0.15294117647058...|\n",
            "|(12,[7],[1.0])|[0.14285714285714...|\n",
            "|(12,[7],[1.0])|[0.14285714285714...|\n",
            "|(12,[7],[1.0])|[0.16279069767441...|\n",
            "|(12,[7],[1.0])|[0.13095238095238...|\n",
            "|(12,[7],[1.0])|[0.13953488372093...|\n",
            "|(12,[7],[1.0])|[0.13095238095238...|\n",
            "|(12,[7],[1.0])|[0.14117647058823...|\n",
            "+--------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idp04MOySm2O",
        "colab_type": "text"
      },
      "source": [
        "So finally, we have our categoryVec column which is the target variable and our features_norm column, which is the feature set for the ML algorithm training.<br>\n",
        "We have seen how to create Apache spark ML Pipelines from our data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kyvAcGDS7L3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}